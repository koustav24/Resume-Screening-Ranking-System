{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Resume Ranking Application\n",
    "\n",
    "This notebook explains the components of a Resume Ranking application that automates the process of screening and ranking candidate resumes against job descriptions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This application:\n",
    "1. Accepts a job description and multiple resume files (PDF format)\n",
    "2. Extracts text from the resume documents\n",
    "3. Validates both the job description and resumes\n",
    "4. Ranks resumes based on their similarity to the job description\n",
    "5. Analyzes key skills and qualifications\n",
    "6. Provides enhanced ranking using advanced NLP techniques\n",
    "7. Visualizes results through a Streamlit web interface\n",
    "\n",
    "The application streamlines the initial resume screening process, helping recruiters identify the most promising candidates."
   ],
   "id": "b8e245e7a7049b65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "\n",
    "def check_and_install_packages():\n",
    "    \"\"\"\n",
    "    Check if required packages are installed, and install them if not.\n",
    "    \"\"\"\n",
    "    required_packages = {\n",
    "        'streamlit': 'For creating the web application interface',\n",
    "        'pandas': 'For data manipulation and analysis',\n",
    "        'PyPDF2': 'For extracting text from PDF files',\n",
    "        'scikit-learn': 'For machine learning utilities including TF-IDF',\n",
    "        'nltk': 'For natural language processing tasks',\n",
    "        'matplotlib': 'For data visualization',\n",
    "        'plotly': 'For interactive visualizations',\n",
    "        'spacy': 'For advanced NLP processing',\n",
    "        'textract': 'For extracting text from various document formats',\n",
    "        'python-docx': 'For handling docx files',\n",
    "        'docx2txt': 'For converting docx to text',\n",
    "        'PyMuPDF': 'Alternative PDF processing library',\n",
    "        'gensim': 'For topic modeling and document similarity',\n",
    "        'sentence-transformers': 'For embeddings and semantic search'\n",
    "    }\n",
    "\n",
    "    for package, purpose in required_packages.items():\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "            print(f\"✓ {package} is already installed: {purpose}\")\n",
    "        except ImportError:\n",
    "            print(f\"Installing {package}: {purpose}\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    # Additional NLTK resources\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "    # Load spaCy model\n",
    "    try:\n",
    "        import spacy\n",
    "        try:\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "            print(\"✓ spaCy model 'en_core_web_sm' is already installed\")\n",
    "        except:\n",
    "            print(\"Installing spaCy model 'en_core_web_sm'\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Run the function to check and install packages\n",
    "check_and_install_packages()\n",
    "\n",
    "# Import required libraries\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"All necessary packages are imported and ready to use!\")"
   ],
   "id": "ed99dea74defd015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    pdf_file : file object\n",
    "        The PDF file to extract text from\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The extracted text from the PDF\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "\n",
    "    try:\n",
    "        # Try using PyPDF2\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text() + \"\\n\"\n",
    "\n",
    "        # If PyPDF2 returns empty or very little text, try alternative methods\n",
    "        if len(text.strip()) < 100:\n",
    "            # Reset file pointer\n",
    "            pdf_file.seek(0)\n",
    "\n",
    "            # Try using PyMuPDF (fitz)\n",
    "            import fitz\n",
    "            doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
    "            for page in doc:\n",
    "                text += page.get_text() + \"\\n\"\n",
    "\n",
    "    except Exception as e:\n",
    "        text = f\"Error extracting text: {str(e)}\"\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "# with open('example_resume.pdf', 'rb') as pdf_file:\n",
    "#     text = extract_text_from_pdf(pdf_file)\n",
    "#     print(f\"Extracted {len(text)} characters from the PDF\")"
   ],
   "id": "d83ae33c631ef2bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_resume(resume_text):\n",
    "    \"\"\"\n",
    "    Validates if the provided text is a valid resume.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    resume_text : str\n",
    "        The text extracted from the resume file\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not resume_text or len(resume_text.strip()) < 100:\n",
    "        return False, \"Resume text is too short or empty\"\n",
    "\n",
    "    # Check for common resume sections\n",
    "    required_sections = [\"experience\", \"education\", \"skills\"]\n",
    "    resume_lower = resume_text.lower()\n",
    "\n",
    "    found_sections = [section for section in required_sections if section in resume_lower]\n",
    "\n",
    "    if len(found_sections) < 2:  # At least 2 sections should be present\n",
    "        return False, \"Resume doesn't appear to have standard sections (experience, education, skills)\"\n",
    "\n",
    "    # Check for contact information patterns\n",
    "    has_email = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', resume_text) is not None\n",
    "    has_phone = re.search(r'(\\+\\d{1,3}\\s?)?(\\(\\d{1,4}\\)|\\d{1,4})[\\s.-]?\\d{3}[\\s.-]?\\d{4}', resume_text) is not None\n",
    "\n",
    "    if not (has_email or has_phone):\n",
    "        return False, \"No contact information (email or phone) found\"\n",
    "\n",
    "    return True, \"\""
   ],
   "id": "6090d6324d0353ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate_job_description(job_description):\n",
    "    \"\"\"\n",
    "    Validates if the provided text is a valid job description.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The text of the job description\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not job_description or len(job_description.strip()) < 100:\n",
    "        return False, \"Job description is too short or empty\"\n",
    "\n",
    "    # Check for common job description sections or terms\n",
    "    job_sections = [\"responsibilities\", \"qualifications\", \"requirements\", \"skills\", \"experience\"]\n",
    "    job_lower = job_description.lower()\n",
    "\n",
    "    found_sections = [section for section in job_sections if section in job_lower]\n",
    "\n",
    "    if len(found_sections) < 2:  # At least 2 sections should be present\n",
    "        return False, \"Job description doesn't have standard sections (responsibilities, qualifications, requirements, etc.)\"\n",
    "\n",
    "    return True, \"\""
   ],
   "id": "b041cfa7255e3b25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def rank_resumes(job_description, resume_texts):\n",
    "    \"\"\"\n",
    "    Ranks resumes based on their similarity to the job description using TF-IDF and cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The job description text\n",
    "    resume_texts : list\n",
    "        List of texts extracted from resume files\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        List of dictionaries containing the similarity score and resume text\n",
    "    \"\"\"\n",
    "    if not resume_texts:\n",
    "        return []\n",
    "\n",
    "    # Preprocess the job description\n",
    "    job_doc = nlp(job_description.lower())\n",
    "    job_tokens = [token.text for token in job_doc if not token.is_stop and not token.is_punct]\n",
    "    processed_job = \" \".join(job_tokens)\n",
    "\n",
    "    # Preprocess the resumes\n",
    "    processed_resumes = []\n",
    "    for resume in resume_texts:\n",
    "        doc = nlp(resume.lower())\n",
    "        tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "        processed_resumes.append(\" \".join(tokens))\n",
    "\n",
    "    # Combine job description and resumes for vectorization\n",
    "    all_documents = [processed_job] + processed_resumes\n",
    "\n",
    "    # Calculate TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_documents)\n",
    "\n",
    "    # Calculate cosine similarity between job description and each resume\n",
    "    job_vector = tfidf_matrix[0:1]\n",
    "    resume_vectors = tfidf_matrix[1:]\n",
    "    cosine_similarities = cosine_similarity(job_vector, resume_vectors)\n",
    "\n",
    "    # Create ranking results\n",
    "    results = []\n",
    "    for i, similarity in enumerate(cosine_similarities[0]):\n",
    "        results.append({\n",
    "            'similarity_score': similarity,\n",
    "            'resume_text': resume_texts[i],\n",
    "            'processed_text': processed_resumes[i]\n",
    "        })\n",
    "\n",
    "    # Sort by similarity score in descending order\n",
    "    results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "\n",
    "    return results"
   ],
   "id": "786ad64d55986691"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_resumes(job_description, uploaded_files):\n",
    "    \"\"\"\n",
    "    Process uploaded resume files and rank them against the job description.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The job description text\n",
    "    uploaded_files : list\n",
    "        List of uploaded resume files\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    tuple\n",
    "        (valid_job, error_message, results, valid_resumes, valid_names)\n",
    "    \"\"\"\n",
    "    # Validate job description\n",
    "    is_valid_job, error_msg = validate_job_description(job_description)\n",
    "\n",
    "    if not is_valid_job:\n",
    "        return is_valid_job, error_msg, [], [], []\n",
    "\n",
    "    valid_resumes = []\n",
    "    valid_names = []\n",
    "\n",
    "    # Process each resume file\n",
    "    for file in uploaded_files:\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            text = extract_text_from_pdf(file)\n",
    "\n",
    "            # Validate resume\n",
    "            is_valid, _ = validate_resume(text)\n",
    "\n",
    "            if is_valid:\n",
    "                valid_resumes.append(text)\n",
    "                valid_names.append(file.name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file.name}: {str(e)}\")\n",
    "\n",
    "    if not valid_resumes:\n",
    "        return is_valid_job, \"No valid resumes found\", [], [], []\n",
    "\n",
    "    # Rank the valid resumes\n",
    "    ranked_results = rank_resumes(job_description, valid_resumes)\n",
    "\n",
    "    # Add file names to results\n",
    "    for i, result in enumerate(ranked_results):\n",
    "        result['file_name'] = valid_names[i]\n",
    "\n",
    "    return is_valid_job, \"\", ranked_results, valid_resumes, valid_names"
   ],
   "id": "a98ec1a4a89c825f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def analyze_key_skills(job_description, resume_texts):\n",
    "    \"\"\"\n",
    "    Analyzes key skills mentioned in the job description and identifies them in resumes.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The job description text\n",
    "    resume_texts : list\n",
    "        List of texts extracted from resume files\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with skill analysis results\n",
    "    \"\"\"\n",
    "    # Extract potential skills from the job description\n",
    "    job_doc = nlp(job_description.lower())\n",
    "\n",
    "    # Get noun chunks and named entities as potential skills\n",
    "    potential_skills = set()\n",
    "    for chunk in job_doc.noun_chunks:\n",
    "        if 2 <= len(chunk.text.split()) <= 4:  # 2-4 word phrases likely to be skills\n",
    "            potential_skills.add(chunk.text)\n",
    "\n",
    "    for ent in job_doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"PRODUCT\", \"WORK_OF_ART\"]:\n",
    "            potential_skills.add(ent.text.lower())\n",
    "\n",
    "    # Add common technical skills by regex pattern matching\n",
    "    tech_patterns = [\n",
    "        r'python|java|c\\+\\+|javascript|react|node\\.js|sql|aws|azure|docker|kubernetes',\n",
    "        r'machine learning|deep learning|natural language processing|computer vision',\n",
    "        r'tensorflow|pytorch|scikit-learn|pandas|numpy',\n",
    "        r'agile|scrum|kanban|project management|leadership'\n",
    "    ]\n",
    "\n",
    "    for pattern in tech_patterns:\n",
    "        for match in re.finditer(pattern, job_description.lower()):\n",
    "            potential_skills.add(match.group(0))\n",
    "\n",
    "    # Filter out very common words or short terms\n",
    "    skill_stopwords = {'experience', 'year', 'work', 'team', 'use', 'using', 'with', 'and', 'the', 'our', 'we'}\n",
    "    filtered_skills = {skill for skill in potential_skills\n",
    "                       if len(skill) > 3 and not any(w in skill_stopwords for w in skill.split())}\n",
    "\n",
    "    # Check each resume for the identified skills\n",
    "    resume_skill_matches = []\n",
    "\n",
    "    for i, resume_text in enumerate(resume_texts):\n",
    "        resume_lower = resume_text.lower()\n",
    "        matched_skills = []\n",
    "\n",
    "        for skill in filtered_skills:\n",
    "            if skill in resume_lower:\n",
    "                matched_skills.append(skill)\n",
    "\n",
    "        match_percentage = len(matched_skills) / len(filtered_skills) if filtered_skills else 0\n",
    "\n",
    "        resume_skill_matches.append({\n",
    "            'resume_index': i,\n",
    "            'matched_skills': matched_skills,\n",
    "            'match_percentage': match_percentage,\n",
    "            'skill_count': len(matched_skills)\n",
    "        })\n",
    "\n",
    "    # Sort by match percentage\n",
    "    resume_skill_matches.sort(key=lambda x: x['match_percentage'], reverse=True)\n",
    "\n",
    "    return {\n",
    "        'identified_skills': list(filtered_skills),\n",
    "        'skill_matches': resume_skill_matches,\n",
    "        'total_skills': len(filtered_skills)\n",
    "    }"
   ],
   "id": "bafc730d9b763294"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def enhanced_resume_ranking(job_description, resume_texts):\n",
    "    \"\"\"\n",
    "    Provides enhanced ranking of resumes using sentence transformers for semantic similarity.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    job_description : str\n",
    "        The job description text\n",
    "    resume_texts : list\n",
    "        List of texts extracted from resume files\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        List of dictionaries containing the enhanced similarity scores\n",
    "    \"\"\"\n",
    "    # Load pre-trained sentence transformer model\n",
    "    try:\n",
    "        model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    except:\n",
    "        # Fallback to standard ranking if model fails to load\n",
    "        print(\"Advanced model couldn't be loaded, falling back to standard ranking\")\n",
    "        return rank_resumes(job_description, resume_texts)\n",
    "\n",
    "    # Create embeddings for job description\n",
    "    job_embedding = model.encode(job_description)\n",
    "\n",
    "    # Create embeddings for resumes\n",
    "    resume_embeddings = model.encode(resume_texts)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for i, resume_embedding in enumerate(resume_embeddings):\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            job_embedding.reshape(1, -1),\n",
    "            resume_embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        similarities.append({\n",
    "            'enhanced_similarity': similarity,\n",
    "            'resume_text': resume_texts[i],\n",
    "            'resume_index': i\n",
    "        })\n",
    "\n",
    "    # Sort by similarity score in descending order\n",
    "    similarities.sort(key=lambda x: x['enhanced_similarity'], reverse=True)\n",
    "\n",
    "    return similarities"
   ],
   "id": "1e4e3e230ccfad90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This is how the Streamlit UI would be implemented\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import io\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"Resume Ranking Application\")\n",
    "    st.write(\"Upload a job description and candidate resumes to rank candidates based on job requirements.\")\n",
    "\n",
    "    # Job Description Input\n",
    "    job_description = st.text_area(\"Enter Job Description:\", height=200)\n",
    "\n",
    "    # Resume Upload\n",
    "    uploaded_files = st.file_uploader(\"Upload Resumes (PDF files):\",\n",
    "                                      type=\"pdf\",\n",
    "                                      accept_multiple_files=True)\n",
    "\n",
    "    if st.button(\"Process Resumes\"):\n",
    "        if job_description and uploaded_files:\n",
    "            # Validate job description and process resumes\n",
    "            is_valid_job, error_msg, processed_results, valid_resumes, valid_names = process_resumes(\n",
    "                job_description, uploaded_files\n",
    "            )\n",
    "\n",
    "            if not is_valid_job:\n",
    "                st.error(f\"Invalid job description: {error_msg}\")\n",
    "            elif not processed_results:\n",
    "                st.error(\"No valid resumes found. Please check your uploaded files.\")\n",
    "            else:\n",
    "                # Display results in a table\n",
    "                results_df = pd.DataFrame(\n",
    "                    [{\n",
    "                        'File Name': r['file_name'],\n",
    "                        'Similarity Score': f\"{r['similarity_score']:.2f}\"\n",
    "                    } for r in processed_results]\n",
    "                )\n",
    "\n",
    "                st.write(\"### Ranking Results\")\n",
    "                st.dataframe(results_df)\n",
    "\n",
    "                # Visualize top results\n",
    "                top_n = 5 if len(processed_results) > 5 else len(processed_results)\n",
    "\n",
    "                fig = px.bar(\n",
    "                    results_df.head(top_n),\n",
    "                    x='File Name',\n",
    "                    y='Similarity Score',\n",
    "                    title=f'Top {top_n} Resume Matches',\n",
    "                    color='Similarity Score'\n",
    "                )\n",
    "                st.plotly_chart(fig)\n",
    "\n",
    "                # Perform enhanced ranking\n",
    "                enhanced_results = enhanced_resume_ranking(job_description, valid_resumes)\n",
    "\n",
    "                # Create dataframe for enhanced results\n",
    "                enhanced_df = pd.DataFrame(\n",
    "                    [{\n",
    "                        'File Name': valid_names[r['resume_index']],\n",
    "                        'Enhanced Score': f\"{r['enhanced_similarity']:.2f}\"\n",
    "                    } for r in enhanced_results]\n",
    "                )\n",
    "\n",
    "                st.write(\"### Enhanced Ranking Results\")\n",
    "                st.dataframe(enhanced_df)\n",
    "\n",
    "                # Skills analysis\n",
    "                st.write(\"### Key Skills Analysis\")\n",
    "                skill_analysis = analyze_key_skills(job_description, valid_resumes)\n",
    "\n",
    "                st.write(f\"**Identified Skills in Job Description:** {len(skill_analysis['identified_skills'])}\")\n",
    "                st.write(\", \".join(skill_analysis['identified_skills']))\n",
    "\n",
    "                # Display skill matches\n",
    "                st.write(\"**Skill Matches by Resume:**\")\n",
    "                for match in skill_analysis['skill_matches']:\n",
    "                    resume_name = valid_names[match['resume_index']]\n",
    "                    st.write(f\"{resume_name}: {match['skill_count']} skills matched \" +\n",
    "                             f\"({match['match_percentage']:.1%})\")\n",
    "\n",
    "        else:\n",
    "            if not job_description:\n",
    "                st.error(\"Please enter a job description\")\n",
    "            if not uploaded_files:\n",
    "                st.error(\"Please upload at least one resume\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check and install packages\n",
    "    check_and_install_packages()\n",
    "\n",
    "    # Run the Streamlit app\n",
    "    main()"
   ],
   "id": "6a0e63f1a53cb13a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Running the Application\n",
    "\n",
    "To run this resume ranking application:\n",
    "\n",
    "1. Make sure all required packages are installed (run the installation cell above)\n",
    "2. Save the complete code in a file named `app.py`\n",
    "3. Run the application using the command:\n",
    "   ```\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "\n",
    "## Example Usage:\n",
    "\n",
    "1. Enter a job description in the text area\n",
    "2. Upload multiple resume PDF files\n",
    "3. Click \"Process Resumes\" to analyze and rank the candidates\n",
    "4. View the ranking results, visualizations, and skills analysis\n",
    "5. Use the information to identify the most promising candidates for further review\n",
    "\n",
    "## Tips:\n",
    "\n",
    "- For best results, use detailed job descriptions that specify required skills and qualifications\n",
    "- Make sure resume PDFs are properly formatted and contain text that can be extracted\n",
    "- The application works best with a reasonable number of resumes (5-20) to compare at once\n",
    "- Both basic TF-IDF and enhanced semantic similarity rankings are provided for comparison"
   ],
   "id": "ddc684d0c4fab9da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
